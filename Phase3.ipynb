{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38532bit6ce7b0d39ed5404aac8d7cf181bbd287",
   "display_name": "Python 3.8.5 32-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon completing Phase 2 of this project, where the initial processed dataset was uploaded to a MySQL database and reviewed with SQL querying, with duplicate rows being deleted, we are now ready to address the issue pertaining to each property's designated district. For a background discussion of this issue, refer back to report published at: https://reggiealderson.github.io/portfolio/blog/nswpropertydataproject/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this phase we will be utilising the NSW Government's Address Location Web Service API, accessible via: http://maps.six.nsw.gov.au/sws/AddressLocation.html\n",
    "\n",
    "The API, if called correctly, will return geographical information (stored in a JSON object) for each property we wish to retrieve information for. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset from the SQL database and saving to a new python dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the relevant modules\n",
    "\n",
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "import os, csv, re\n",
    "import urllib.parse\n",
    "import urllib.request\n",
    "import urllib.error\n",
    "import json\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = sqlalchemy.create_engine('mysql+pymysql://root:learningSQL@localhost:3306/testdatabase') #This allows python to interact with your MySQL database. # Tutorial to be watched to understand this line of code: https://www.youtube.com/watch?v=M-4EpNdlSuY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_sql_table(\"property_sales\", engine) #This reads the dataset from the property_sales table in the MySQL database, and saves a copy as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Number of rows, number of columns: \n(28651, 16)\n\n\nFirst five rows of the dataset: \n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   district_code  property_id property_unit_number property_house_number  \\\n0              1    4163208.0                 None                     1   \n1              1       4037.0                 None                   332   \n2              1       2170.0                 None                    67   \n3              1       6554.0                 None                     1   \n4              1       6621.0                 None                    63   \n\n  property_street_name property_locality  property_post_code     area  \\\n0        ST ANDREWS CL      HEDDON GRETA                2321   782.10   \n1          WOLLOMBI RD  BELLBIRD HEIGHTS                2325   809.40   \n2           CHARLES ST          ABERMAIN                2326  6373.00   \n3             EDITH ST          CESSNOCK                2325   809.43   \n4          FERGUSON ST          CESSNOCK                2325   696.77   \n\n  contract_date settlement_date  purchase_price zoning_classification  \\\n0    2020-01-14      2020-02-25          240000                    R2   \n1    2020-01-28      2020-02-27          275000                    R2   \n2    2020-02-05      2020-02-21          200000                    R5   \n3    2020-01-24      2020-02-24          290000                    R3   \n4    2020-01-29      2020-02-26          300000                    R3   \n\n   is_unit  is_house record_id  id  \n0      NaN       1.0  AP924668   1  \n1      NaN       1.0  AP927983   2  \n2      NaN       1.0  AP917863   3  \n3      NaN       1.0  AP921667   4  \n4      NaN       1.0  AP926464   5  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>district_code</th>\n      <th>property_id</th>\n      <th>property_unit_number</th>\n      <th>property_house_number</th>\n      <th>property_street_name</th>\n      <th>property_locality</th>\n      <th>property_post_code</th>\n      <th>area</th>\n      <th>contract_date</th>\n      <th>settlement_date</th>\n      <th>purchase_price</th>\n      <th>zoning_classification</th>\n      <th>is_unit</th>\n      <th>is_house</th>\n      <th>record_id</th>\n      <th>id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>4163208.0</td>\n      <td>None</td>\n      <td>1</td>\n      <td>ST ANDREWS CL</td>\n      <td>HEDDON GRETA</td>\n      <td>2321</td>\n      <td>782.10</td>\n      <td>2020-01-14</td>\n      <td>2020-02-25</td>\n      <td>240000</td>\n      <td>R2</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>AP924668</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>4037.0</td>\n      <td>None</td>\n      <td>332</td>\n      <td>WOLLOMBI RD</td>\n      <td>BELLBIRD HEIGHTS</td>\n      <td>2325</td>\n      <td>809.40</td>\n      <td>2020-01-28</td>\n      <td>2020-02-27</td>\n      <td>275000</td>\n      <td>R2</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>AP927983</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>2170.0</td>\n      <td>None</td>\n      <td>67</td>\n      <td>CHARLES ST</td>\n      <td>ABERMAIN</td>\n      <td>2326</td>\n      <td>6373.00</td>\n      <td>2020-02-05</td>\n      <td>2020-02-21</td>\n      <td>200000</td>\n      <td>R5</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>AP917863</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>6554.0</td>\n      <td>None</td>\n      <td>1</td>\n      <td>EDITH ST</td>\n      <td>CESSNOCK</td>\n      <td>2325</td>\n      <td>809.43</td>\n      <td>2020-01-24</td>\n      <td>2020-02-24</td>\n      <td>290000</td>\n      <td>R3</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>AP921667</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>6621.0</td>\n      <td>None</td>\n      <td>63</td>\n      <td>FERGUSON ST</td>\n      <td>CESSNOCK</td>\n      <td>2325</td>\n      <td>696.77</td>\n      <td>2020-01-29</td>\n      <td>2020-02-26</td>\n      <td>300000</td>\n      <td>R3</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>AP926464</td>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 106
    }
   ],
   "source": [
    "#Checking the dataset was read correctly from the MySQL table\n",
    "\n",
    "print('Number of rows, number of columns: ')\n",
    "print(dataset.shape)\n",
    "print('\\n')\n",
    "print('First five rows of the dataset: ')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data transformation (prior to use of API)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will loop through each property transaction (row in the dataframe), and derive a new column which contains a clean string of the street name a property belongs to. This is necessary for a couple of reasons. Firstly, the raw dataset is fairly inconsistent with the structure of the street name data point across all of the property transactions. For instance, some records will mistakenly have numbers and leading white spacesd in the street name, while others may contain duplicate road type strings (e.g. 'CHARLES ROAD RD'). Secondly, the API we will be using requires that we enter a road name parameter, which only accepts the road name (not including the road type). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below cell, I have referenced two csv files which contain road name types and abbreviations (e.g. 'RD', 'CRES', 'STREET', 'AVENUE', 'LNWY'). If these types and abbreviations are included in the road name parameter when calling the API, then the returned JSON object will contain no information. The csv files are available here: https://github.com/reggiealderson/NSWpropertysales . The choice as to which words and abbreviations to include in these csv files was difficult. For instance, do I include a word like 'TERRACE' which could be both a road type and also a legitimate word in a street name (e.g. RAYMOND TERRACE ROAD). I had to do several trial runs with the API, checking the dataset with SQL queries to see the prevalence of certain abbreviations and road type words in the dataset, and then updating the csv lists, before I could be happy with the % of missing values generated from the API call. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up python lists that will be eventually looped through for each property record\n",
    "\n",
    "abbrevfile_obj = open('abbrev.csv')\n",
    "typesfile_obj = open('types.csv')\n",
    "abbrevreader_obj = csv.reader(abbrevfile_obj)\n",
    "typesreader_obj = csv.reader(typesfile_obj)\n",
    "\n",
    "listofabbrev = list(abbrevreader_obj)\n",
    "listofabbrev[0][0] = 'ACCS' #This is just to fix up the first entry as for some reason it gets read wrong by the csv reader. \n",
    "listofabbrev = listofabbrev[0] #Refers to the first row in the csv file.\n",
    "\n",
    "listoftypes = list(typesreader_obj)\n",
    "listoftypes[0][0] = 'ACCESS' #This is just to fix up the first entry as for some reason it gets read wrong by the csv reader. \n",
    "listoftypes = listoftypes[0] #Refers to the first row in the csv file.\n",
    "\n",
    "abbrevfile_obj.close()\n",
    "typesfile_obj.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [15,3,4,5] # Refering to the columns in the dataset that we need information for. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties_list = dataset[dataset.columns[cols]] # (1 of 2) Creates a python list containing the property data we will be transforming and then subsequently passing through the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties_list = properties_list.values.tolist() # (2 of 2) Creates a python list containing the property data we will be transforming and then subsequently passing through the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[22062, None, 'AERODROME ROAD RD', 'LIGHTNING RIDGE']"
     },
     "metadata": {},
     "execution_count": 131
    }
   ],
   "source": [
    "properties_list[17974] # A random check of property data (use as reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing suffixes such as 'ST', 'RD', etc from the street name strings -- so that the future API calls can function properly\n",
    "# This cell should take around 5 seconds to process.\n",
    "\n",
    "for i in properties_list:\n",
    "    counter = 0\n",
    "    sec_counter = 0\n",
    "    stn1 = i[2] # Refers to the street name\n",
    "    stn2 = ''.join([i for i in stn1 if not i.isdigit()]).lstrip() # Gets rid of digits and leading white spaces\n",
    "    stn3 = re.sub(r'(?:^| )\\w(?:$| )', ' ', stn2).strip() # Gets rid of singular characters in the string \n",
    "    stn4 = stn3 + ' ' \n",
    "    for idx, abbrev in enumerate(listofabbrev):\n",
    "        ab1 = ' ' + abbrev + ' ' # creates a string for each possible abbreviation, with white spaces on either side so that when searching for the string it is not confused with a word which contains the abbreviation (e.g. the 'ST' abbreviation features in the word 'STUART')\n",
    "        stn5 = re.search(f\"{ab1}$\", stn4) # Here we are searching for the abbreviation in the property's street name\n",
    "        if(stn5!=None): # If this IF condition is met, then the abbreviation features in the property's street name and a couple of variables are created (stored in memory only)\n",
    "            counter += 1\n",
    "            next_idx_pos = idx + 1\n",
    "            ab2 = ab1 \n",
    "            ab3 =  ' ' + abbrev\n",
    "            f_abbrev = abbrev\n",
    "            if stn3 == 'THE' + ab3:\n",
    "                i.append(stn3)\n",
    "            else: # For all other circumstances where the abbreviation is featured in the street name, it is removed and a new variable is added to the properties_list.\n",
    "                ab4 = ' ' + f_abbrev + ' ' + f_abbrev\n",
    "                stn6 = re.search(f\"{ab4}$\", stn3)\n",
    "                if(stn6!=None): #If there is a repeated abbreviation (e.g 'RD RD')\n",
    "                    var1 = stn3.replace(ab4, '')\n",
    "                    i.append(var1)\n",
    "                else:\n",
    "                    var2 = stn4.replace(ab2, '')\n",
    "                    for abbrev2 in listofabbrev[next_idx_pos:]: #checks if there is a second distinct abbreviation\n",
    "                        ab1a = ' ' + abbrev2\n",
    "                        stn5a = re.search(f\"{ab1a}$\", var2)\n",
    "                        if(stn5a!=None): # if there is a second distinct abbreviation\n",
    "                            var3 = var2.replace(ab1a, '')\n",
    "                            i.append(var3)\n",
    "                            sec_counter += 1\n",
    "                        else:\n",
    "                            continue\n",
    "                    if sec_counter == 0: # if there is not a second distinct abbreviation, it falls back to var2\n",
    "                        i.append(var2)\n",
    "                    else:\n",
    "                        continue\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    if counter == 0: #if there were no abbreviations then a new variable is still added to the properties_list, it is just a copy of the original street name string. \n",
    "        i.append(stn3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[22062, None, 'AERODROME ROAD RD', 'LIGHTNING RIDGE', 'AERODROME ROAD']"
     },
     "metadata": {},
     "execution_count": 133
    }
   ],
   "source": [
    "# Running this cell you will now see that a new data point is added for each property in properties_list. The new data point represents a semi cleaned street name string. However we still need to create a script that will handle strings such as 'ROAD', 'AVENUE', etc.\n",
    "\n",
    "properties_list[17974]\n",
    "# properties_list[108]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[3901, '2', 'BELLCAST RD BUILDING A', 'ROUSE HILL', 'BELLCAST']"
     },
     "metadata": {},
     "execution_count": 134
    }
   ],
   "source": [
    "properties_list[3900] # Another random check of property data (use as reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing instances where the street name string ends in a substring like \"ROAD\" or \"STREET\" -- so that the future API calls can functions properly\n",
    "\n",
    "for i in properties_list:\n",
    "    tt1 = i[4] #refers to the previously created string holding the semi cleaned street name\n",
    "    occureda = 0 \n",
    "    occuredb = 0 \n",
    "    occuredc = 0 \n",
    "    for typ in listoftypes:\n",
    "        tt2 = ' ' + typ\n",
    "        tt3 = re.search(f\"{tt2}$\", tt1)\n",
    "        if(tt3!=None):\n",
    "            for typ2 in listoftypes:\n",
    "                pp1 = typ2 + tt2\n",
    "                if tt1 == pp1:\n",
    "                    i.append(tt1)\n",
    "                    occureda += 1\n",
    "                else:\n",
    "                    continue\n",
    "            if tt1.startswith(\"THE \"): #This condition is here in case the street name starts with 'The'. For instance if the street name is \"THE GREAT OCEAN ROAD\", then the API will actually need the 'ROAD' part of the string to be entered into the street name parameter. Whereas in all other cases (e.g. 'BAKERS ROAD') the API requires the 'ROAD' part to be excluded in the parameter.\n",
    "                i.append(tt1)\n",
    "                occuredb += 1 \n",
    "            else:\n",
    "                rr1 = tt1.replace(tt2, '')\n",
    "                i.append(rr1)\n",
    "                occuredc += 1 \n",
    "        else:\n",
    "            continue\n",
    "    if ((occureda == 0) and (occuredb == 0) and (occuredc == 0)): \n",
    "        i.append(tt1)\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[109, None, 'WILLIAM ST ST', 'PAXTON', 'WILLIAM', 'WILLIAM']"
     },
     "metadata": {},
     "execution_count": 136
    }
   ],
   "source": [
    "# A random check of property data (use as reference)\n",
    "properties_list[108]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[22062,\n None,\n 'AERODROME ROAD RD',\n 'LIGHTNING RIDGE',\n 'AERODROME ROAD',\n 'AERODROME']"
     },
     "metadata": {},
     "execution_count": 137
    }
   ],
   "source": [
    "properties_list[17974] # A random check of property data (use as reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[1067,\n '24',\n 'THE RIDGE ROAD',\n 'EAST MAITLAND',\n 'THE RIDGE ROAD',\n 'THE RIDGE ROAD']"
     },
     "metadata": {},
     "execution_count": 138
    }
   ],
   "source": [
    "# In this example, 'ROAD' is kept in the final cleaned street name variable - because the street name starts with 'THE' - making it a special case.\n",
    "properties_list[1066]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interacting with the geo-coding API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned, we will be using the NSW Government's Address Location Web Service API in order to gain certain geographical data for each property we pass through to the API. In our case, we wish to retrieve a property's designated district name, and it's longitude and latitude coordinates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "28651"
     },
     "metadata": {},
     "execution_count": 139
    }
   ],
   "source": [
    "len(properties_list) # Checking we have the correct amount of properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I recommend doing a test run of the API with a much smaller dataset. Here I've created a list 'testrun_list' which you can replace with 'properties_list' in the next cell once you are satisfied that the API call will run smoothly.\n",
    "\n",
    "testrun_list = properties_list[0:150] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The actual API call\n",
    "\n",
    "The next cell runs the API call. Note this took my PC roughly 45 minutes to process with a sample of 28,651 properties. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Time elapsed (hh:mm:ss.ms) 0:00:35.077087\n"
    }
   ],
   "source": [
    "start_time = datetime.now()\n",
    "BASE_URL = 'http://maps.six.nsw.gov.au/services/public/Address_Location'\n",
    "current_delay = 0.1  # Set the initial retry delay to 100ms.\n",
    "max_delay = 5  # Set the maximum retry delay to 5 seconds.\n",
    "count_of_failed_data_retrieval = 0\n",
    "count_of_failed_urlaccess = 0\n",
    "\n",
    "for i in properties_list: # Here is where you substitute your list if you are looking to do a test run with a smaller dataset\n",
    "    roadName = i[5].replace(\" \", \"+\")\n",
    "    suburb = i[3].replace(\" \", \"+\")\n",
    "    if i[1] != '':\n",
    "        houseNumber_uncleaned = i[1]\n",
    "        houseNumber_uncleaned_str = str(houseNumber_uncleaned)\n",
    "        housenumber_numericonly = re.sub(\"[^0-9]\", \"\", houseNumber_uncleaned_str)\n",
    "        houseNumber = housenumber_numericonly.replace(\" \", \"+\")\n",
    "    else:\n",
    "        houseNumber = ''\n",
    "        \n",
    "    url = f\"{BASE_URL}?houseNumber={houseNumber}&roadName={roadName}&suburb={suburb}&projection=EPSG%3A4326\"\n",
    "    while True:\n",
    "        try:\n",
    "            result = json.load(urllib.request.urlopen(url)) # Get the API response.\n",
    "        except urllib.error.URLError:\n",
    "            count_of_failed_urlaccess += 1\n",
    "            pass  # Fall through to the retry loop\n",
    "        else:\n",
    "            try:\n",
    "                result_district = result['addressResult']['addresses'][0]['council']\n",
    "                resultx = result['addressResult']['addresses'][0]['addressPoint']['centreX']\n",
    "                resulty = result['addressResult']['addresses'][0]['addressPoint']['centreY']\n",
    "                i.append(result_district)\n",
    "                i.append(resultx)\n",
    "                i.append(resulty)\n",
    "            except KeyError:\n",
    "                count_of_failed_data_retrieval += 1\n",
    "            break\n",
    "\n",
    "        if current_delay > max_delay:\n",
    "            raise Exception(\"Too many retry attempts.\")\n",
    "\n",
    "        time.sleep(current_delay)\n",
    "        current_delay *= 2  # Increase the delay each time we retry.\n",
    "\n",
    "print('Time elapsed (hh:mm:ss.ms) {}'.format(datetime.now() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess the number of properties which did not return geocoding data:\n",
    "\n",
    "listofnonres = [x for x in properties_list if len(x) == 6]\n",
    "print('Number of properties which did not return geocording data is: ')\n",
    "len(listofnonres)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing the geocoding data in MySQL database\n",
    "\n",
    "Here we will load the new data to MySQL as a new table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties_list_geo = properties_list.values.tolist() #Reducing the amount of data we want to upload to SQL databae to only the new geocoding data (+ id column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Picking out only the new geocoding data + the cleaned street name, and id (for reference)\n",
    "\n",
    "dataset_onlygeocoded = [] \n",
    "for x in properties_list_geo:\n",
    "    indexes = [0,5,6,7,8] \n",
    "    pickout = [x[y] for y in indexes]\n",
    "    dataset_onlygeocoded.append(pickout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_withgeocoding = pd.DataFrame(dataset_onlygeocoded) #creating a pandas dataframe that is used a medium to transfer our dataset to MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = sqlalchemy.create_engine('mysql+pymysql://root:password@hostname/databasename') #This allows python to interact with your MySQL database. # Tutorial to be watched to understand this line of code: https://www.youtube.com/watch?v=M-4EpNdlSuY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names2 = ['id', 'cleaned_street_name', 'district', 'longitude', 'latitude']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_withgeocoding.columns = column_names2 #setting up the column names in the python dataframe so the dataset will be accepted by MySQL. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a new table in the MySQL database and load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter the following SQL query in MySQL Workbench:\n",
    "\n",
    "''' \n",
    "\n",
    "CREATE TABLE `testdatabase`.`geocoding_data` (\n",
    "  `id` INT NOT NULL,\n",
    "  `cleaned_street_name` VARCHAR(150) NULL,\n",
    "  `district` VARCHAR(100) NULL,\n",
    "  `longitude` VARCHAR(100) NULL,\n",
    "  `latitude` VARCHAR(100) NULL,\n",
    "  PRIMARY KEY (`id`),\n",
    "  UNIQUE INDEX `id_UNIQUE` (`id` ASC) VISIBLE);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store the geocoding data in the new table:\n",
    "\n",
    "df_withgeocoding.to_sql(name='geocoding_data', index=False, con=engine, if_exists='append')"
   ]
  }
 ]
}